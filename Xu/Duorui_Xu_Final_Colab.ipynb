{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5386d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import hashlib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, brier_score_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "FILE_PATHS = [\n",
    "    \"/content/drive/MyDrive/CS441/Dallas_Animal_Shelter_Data_Fiscal_Year_2023_-_2026.csv\",\n",
    "    \"/content/drive/MyDrive/CS441/dogs_intake_outcome_2021_2025.xlsx\",\n",
    "]\n",
    "\n",
    "ADOPTION_ONLY = True\n",
    "K_LIST = [7, 14, 30]\n",
    "SEED = 42\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 80\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTHING = 0.05\n",
    "N_RUNS = 10\n",
    "NUM_WORKERS = min(8, os.cpu_count() or 4)\n",
    "TOP_K_FS = 256\n",
    "TOP_N_BREEDS = 20\n",
    "USE_BERT = True\n",
    "BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "BERT_MAX_LENGTH = 64\n",
    "BERT_BATCH_SIZE = 64\n",
    "BERT_USE_AMP = True\n",
    "BERT_CACHE_DIR = \"_bert_cache\"\n",
    "TEXT_COLS_FOR_BERT = [\"Animal_Breed\", \"Intake_Condition\", \"Reason\", \"Hold_Request\"]\n",
    "BERT_GPU_ID = 0\n",
    "TRAIN_GPU_ID = 0\n",
    "RESULTS_TXT_NAME = \"surv_binary3_final_only.txt\"\n",
    "AGE_DAYS_MAP = {\"puppy\": 0.5 * 365.0, \"adult\": 5.0 * 365.0, \"senior\": 10.5 * 365.0}\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def pick_cuda_device(gpu_id: int) -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        n = torch.cuda.device_count()\n",
    "        if n == 0:\n",
    "            return torch.device(\"cpu\")\n",
    "        if gpu_id < 0 or gpu_id >= n:\n",
    "            print(f\"[WARN] gpu_id={gpu_id} out of range (0..{n-1}), fallback cuda:0\")\n",
    "            gpu_id = 0\n",
    "        return torch.device(f\"cuda:{gpu_id}\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def print_gpu_info():\n",
    "    print(\"[HW] torch.cuda.is_available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[HW] torch.cuda.device_count:\", torch.cuda.device_count())\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"[HW] GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "def now_hms() -> str:\n",
    "    return time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "def status(msg: str) -> None:\n",
    "    print(f\"[{now_hms()}] {msg}\")\n",
    "\n",
    "def write_final_line(results_txt_path: str, msg: str) -> None:\n",
    "    with open(results_txt_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(msg + \"\\n\")\n",
    "\n",
    "def read_any_table(path: str) -> pd.DataFrame:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xls\"]:\n",
    "        return pd.read_excel(path)\n",
    "    if ext == \".csv\":\n",
    "        return pd.read_csv(path, low_memory=False)\n",
    "    raise ValueError(f\"Unsupported file type: {ext}, path={path}\")\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    if \"Animal_Type\" in out.columns:\n",
    "        out[\"Animal_Type\"] = out[\"Animal_Type\"].astype(str).str.upper().str.strip()\n",
    "    elif \"Species\" in out.columns:\n",
    "        out[\"Animal_Type\"] = out[\"Species\"].astype(str).str.upper().str.strip()\n",
    "    else:\n",
    "        out[\"Animal_Type\"] = \"UNKNOWN\"\n",
    "\n",
    "    if \"Animal_Breed\" in out.columns:\n",
    "        out[\"Animal_Breed\"] = out[\"Animal_Breed\"].astype(str)\n",
    "    else:\n",
    "        prim = out.get(\"Primary Breed\", pd.Series([\"\"] * len(out))).fillna(\"\").astype(str).str.strip()\n",
    "        sec = out.get(\"Secondary Breed\", pd.Series([\"\"] * len(out))).fillna(\"\").astype(str).str.strip()\n",
    "        breed = prim.copy()\n",
    "        mask = sec.ne(\"\") & prim.ne(\"\") & sec.ne(prim)\n",
    "        breed.loc[mask] = prim.loc[mask] + \" / \" + sec.loc[mask]\n",
    "        mask2 = prim.eq(\"\") & sec.ne(\"\")\n",
    "        breed.loc[mask2] = sec.loc[mask2]\n",
    "        out[\"Animal_Breed\"] = breed.replace(\"\", \"Unknown\")\n",
    "\n",
    "    if \"Intake_Date\" in out.columns:\n",
    "        out[\"Intake_Date\"] = pd.to_datetime(out[\"Intake_Date\"], errors=\"coerce\")\n",
    "    elif \"Intake Date\" in out.columns:\n",
    "        out[\"Intake_Date\"] = pd.to_datetime(out[\"Intake Date\"], errors=\"coerce\")\n",
    "    elif \"Intake_DateTime\" in out.columns:\n",
    "        out[\"Intake_Date\"] = pd.to_datetime(out[\"Intake_DateTime\"], errors=\"coerce\")\n",
    "    else:\n",
    "        out[\"Intake_Date\"] = pd.NaT\n",
    "\n",
    "    if \"Outcome_Date\" in out.columns:\n",
    "        out[\"Outcome_Date\"] = pd.to_datetime(out[\"Outcome_Date\"], errors=\"coerce\")\n",
    "    elif \"Outcome Date\" in out.columns:\n",
    "        out[\"Outcome_Date\"] = pd.to_datetime(out[\"Outcome Date\"], errors=\"coerce\")\n",
    "    else:\n",
    "        out[\"Outcome_Date\"] = pd.NaT\n",
    "\n",
    "    if \"Outcome_Type\" in out.columns:\n",
    "        out[\"Outcome_Type\"] = out[\"Outcome_Type\"].astype(str)\n",
    "    elif \"Outcome Type\" in out.columns:\n",
    "        out[\"Outcome_Type\"] = out[\"Outcome Type\"].astype(str)\n",
    "    else:\n",
    "        out[\"Outcome_Type\"] = \"UNKNOWN\"\n",
    "    out[\"Outcome_Type\"] = out[\"Outcome_Type\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "    if \"Intake_Type\" in out.columns:\n",
    "        out[\"Intake_Type\"] = out[\"Intake_Type\"].astype(str)\n",
    "    elif \"Intake Type\" in out.columns:\n",
    "        out[\"Intake_Type\"] = out[\"Intake Type\"].astype(str)\n",
    "\n",
    "    if \"Intake_Condition\" in out.columns:\n",
    "        out[\"Intake_Condition\"] = out[\"Intake_Condition\"].fillna(\"\").astype(str)\n",
    "    else:\n",
    "        out[\"Intake_Condition\"] = \"\"\n",
    "\n",
    "    out[\"Reason\"] = out.get(\"Reason\", \"\").fillna(\"\").astype(str) if \"Reason\" in out.columns else \"\"\n",
    "    out[\"Hold_Request\"] = out.get(\"Hold_Request\", \"\").fillna(\"\").astype(str) if \"Hold_Request\" in out.columns else \"\"\n",
    "\n",
    "    if \"AgeGroup\" in out.columns:\n",
    "        out[\"AgeGroup\"] = out[\"AgeGroup\"].fillna(\"\").astype(str)\n",
    "    elif \"Age Group at Intake\" in out.columns:\n",
    "        raw = out[\"Age Group at Intake\"].fillna(\"\").astype(str).str.upper()\n",
    "        def _map_age(s: str) -> str:\n",
    "            if \"PUPPY\" in s or \"UNWEANED\" in s:\n",
    "                return \"puppy\"\n",
    "            if \"SENIOR\" in s:\n",
    "                return \"senior\"\n",
    "            if \"ADULT\" in s:\n",
    "                return \"adult\"\n",
    "            return \"adult\"\n",
    "        out[\"AgeGroup\"] = raw.apply(_map_age)\n",
    "    else:\n",
    "        out[\"AgeGroup\"] = \"\"\n",
    "\n",
    "    if \"Days in Custody\" in out.columns:\n",
    "        out[\"Days_in_Custody\"] = pd.to_numeric(out[\"Days in Custody\"], errors=\"coerce\")\n",
    "    elif \"StayLength\" in out.columns:\n",
    "        out[\"Days_in_Custody\"] = pd.to_numeric(out[\"StayLength\"], errors=\"coerce\")\n",
    "    else:\n",
    "        out[\"Days_in_Custody\"] = np.nan\n",
    "\n",
    "    return out\n",
    "\n",
    "def load_and_merge_sources(paths: List[str]) -> Tuple[pd.DataFrame, str]:\n",
    "    existing = [p for p in paths if p and os.path.isfile(p)]\n",
    "    if not existing:\n",
    "        raise FileNotFoundError(\"No valid file in FILE_PATHS. Please check paths.\")\n",
    "    out_dir = os.path.dirname(existing[0]) if os.path.dirname(existing[0]) else os.getcwd()\n",
    "    dfs = []\n",
    "    for p in existing:\n",
    "        status(f\"[LOAD] reading: {p}\")\n",
    "        raw = read_any_table(p)\n",
    "        std = standardize_columns(raw)\n",
    "        std[\"__source__\"] = os.path.basename(p)\n",
    "        dfs.append(std)\n",
    "    df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    status(f\"[LOAD] merged rows={len(df)} from {len(existing)} files\")\n",
    "    return df, out_dir\n",
    "\n",
    "HERDING_KEYS = {\"SHEPHERD\",\"SHEEPDOG\",\"MALINOIS\",\"CATTLE\",\"HEELER\",\"BORDER COLLIE\",\"KELPIE\",\"COLLIE\",\"CORGI\",\"AUSSIE\"}\n",
    "WORKING_KEYS = {\"ROTTWEILER\",\"ROTT\",\"MASTIFF\",\"PYRENEES\",\"ST BERNARD\",\"SAINT BERNARD\",\"BERNESE\",\"BOXER\",\"DOBERMAN\",\"DOBERMANN\",\n",
    "                \"NEWFOUNDLAND\",\"GREAT DANE\",\"AKITA\",\"RIDGEBACK\",\"CANE CORSO\",\"PRESA\",\"BULLMASTIFF\",\"KOMONDOR\"}\n",
    "SPORTING_KEYS = {\"LABRADOR\",\"GOLDEN\",\"RETRIEVER\",\"SETTER\",\"POINTER\",\"SPANIEL\",\"WEIMARANER\",\"VIZSLA\",\"BRITTANY\",\"GRIFFON\"}\n",
    "NONSPORTING_KEYS = {\"POODLE\",\"BULLDOG\",\"DALMATIAN\",\"BICHON\",\"BOSTON\",\"FRENCH\",\"CHOW\",\"SHIBA\",\"SHAR PEI\",\"KEESHOND\"}\n",
    "TOY_KEYS = {\"CHIHUAHUA\",\"POMERANIAN\",\"YORKSHIRE\",\"YORKIE\",\"SHIH\",\"MALTESE\",\"PUG\",\"PAPILLON\",\n",
    "            \"MIN PINSCHER\",\"MINIATURE PINSCHER\",\"MINI PIN\",\"TOY POODLE\",\"MINIATURE POODLE\",\"PEKINGESE\"}\n",
    "TERRIER_KEYS = {\"TERRIER\",\"PIT\",\"STAFFORDSHIRE\",\"STAFFY\",\"BULL TERRIER\",\"AIREDALE\",\"FOX TERRIER\",\"SCOTTISH\",\"WEST HIGHLAND\",\"JACK RUSSELL\"}\n",
    "HOUND_KEYS = {\"HOUND\",\"BEAGLE\",\"BASSET\",\"GREYHOUND\",\"DACHSHUND\",\"AFGHAN\",\"BLOODHOUND\",\"COONHOUND\",\"RHODESIAN\",\"RIDGEBACK\"}\n",
    "\n",
    "FIERCE_KEYS = [\n",
    "    \"PIT BULL\",\"PITBULL\",\"AMERICAN PIT\",\"AM PIT\",\"AMERICAN STAFFORDSHIRE\",\"STAFFORDSHIRE TERRIER\",\"AM STAFF\",\n",
    "    \"BULL TERRIER\",\"MINIATURE BULL TERRIER\",\"ROTTWEILER\",\"ROTT\",\"DOBERMAN\",\"DOBERMANN\",\"MASTIFF\",\"BULLMASTIFF\",\n",
    "    \"CANE CORSO\",\"PRESA CANARIO\",\"DOGO ARGENTINO\",\"TOSA\",\"FILA BRASILEIRO\",\"KANGAL\",\"RIDGEBACK\",\"RHODESIAN RIDGEBACK\",\"AKITA\",\n",
    "]\n",
    "\n",
    "GIANT_KEYS = {\"GREAT DANE\",\"MASTIFF\",\"PYRENEES\",\"ST BERNARD\",\"SAINT BERNARD\",\"NEWFOUNDLAND\",\"IRISH WOLFHOUND\"}\n",
    "LARGE_KEYS = {\"ROTTWEILER\",\"ROTT\",\"GERMAN SHEPHERD\",\"SHEPHERD\",\"LABRADOR\",\"GOLDEN\",\"MALINOIS\",\"BOXER\",\"DOBERMAN\",\"RIDGEBACK\",\"AKITA\",\"HUSKY\",\"MALAMUTE\"}\n",
    "SMALL_KEYS = {\"CHIHUAHUA\",\"POMERANIAN\",\"YORKSHIRE\",\"YORKIE\",\"SHIH\",\"MALTESE\",\"PUG\",\"PAPILLON\",\"DACHSHUND\",\"JACK RUSSELL\",\n",
    "              \"MINIATURE\",\"MINI \",\"TOY\",\"BOSTON\",\"BEAGLE\",\"CAVALIER\",\"PEKINGESE\"}\n",
    "\n",
    "PUPPY_WORDS = {\"UNDERAGE\", \"PUPPY\", \"JUVENILE\", \"BABY\", \"NEONATE\"}\n",
    "SENIOR_WORDS = {\"GERIATRIC\", \"SENIOR\", \"OLD\", \"AGED\"}\n",
    "\n",
    "HEALTH_TOKENS = {\n",
    "    \"healthy\": {\"APP\", \"WNL\", \"NORMAL\"},\n",
    "    \"injured\": {\"INJ\", \"INJURED\"},\n",
    "    \"sick\": {\"SICK\"},\n",
    "    \"critical\": {\"CRITICAL\"},\n",
    "    \"dead\": {\"DECEASED\", \"FATAL\"},\n",
    "    \"underage\": {\"UNDERAGE\"},\n",
    "    \"geriatric\": {\"GERIATRIC\"},\n",
    "}\n",
    "\n",
    "def map_breed_use(breed: str) -> str:\n",
    "    if not isinstance(breed, str) or not breed:\n",
    "        return \"unknown\"\n",
    "    s = breed.upper()\n",
    "    if any(k in s for k in HERDING_KEYS): return \"herding\"\n",
    "    if any(k in s for k in WORKING_KEYS): return \"working\"\n",
    "    if any(k in s for k in SPORTING_KEYS): return \"sporting\"\n",
    "    if any(k in s for k in TOY_KEYS): return \"toy\"\n",
    "    if any(k in s for k in TERRIER_KEYS): return \"terrier\"\n",
    "    if any(k in s for k in HOUND_KEYS): return \"hound\"\n",
    "    if any(k in s for k in NONSPORTING_KEYS): return \"nonsporting\"\n",
    "    if \"MIX\" in s or \"MIXED\" in s: return \"mixed_unknown\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def map_breed_size(breed: str) -> str:\n",
    "    if not isinstance(breed, str) or not breed:\n",
    "        return \"unknown\"\n",
    "    s = breed.upper()\n",
    "    if \"GIANT\" in s: return \"giant\"\n",
    "    if \"TOY\" in s or \"MINIATURE\" in s or \"MINI \" in s: return \"small\"\n",
    "    if any(k in s for k in GIANT_KEYS): return \"giant\"\n",
    "    if any(k in s for k in LARGE_KEYS): return \"large\"\n",
    "    if any(k in s for k in SMALL_KEYS): return \"small\"\n",
    "    return \"medium\"\n",
    "\n",
    "def map_breed_temper(breed: str) -> str:\n",
    "    if not isinstance(breed, str) or not breed:\n",
    "        return \"unknown\"\n",
    "    s = breed.upper()\n",
    "    return \"fierce\" if any(key in s for key in FIERCE_KEYS) else \"normal\"\n",
    "\n",
    "def tokenize_upper(s: str) -> set:\n",
    "    if not isinstance(s, str):\n",
    "        return set()\n",
    "    toks = re.split(r\"[^A-Z]+\", s.upper())\n",
    "    return {t for t in toks if t}\n",
    "\n",
    "def normalize_intake_condition(cond: str) -> str:\n",
    "    toks = tokenize_upper(cond)\n",
    "    if not toks: return \"unknown\"\n",
    "    if HEALTH_TOKENS[\"dead\"] & toks: return \"dead\"\n",
    "    if HEALTH_TOKENS[\"critical\"] & toks: return \"critical\"\n",
    "    if HEALTH_TOKENS[\"injured\"] & toks: return \"injured\"\n",
    "    if HEALTH_TOKENS[\"sick\"] & toks: return \"sick\"\n",
    "    if HEALTH_TOKENS[\"underage\"] & toks: return \"underage\"\n",
    "    if HEALTH_TOKENS[\"geriatric\"] & toks: return \"geriatric\"\n",
    "    if HEALTH_TOKENS[\"healthy\"] & toks: return \"healthy\"\n",
    "    return \"other\"\n",
    "\n",
    "def infer_age_group_from_condition(cond: str) -> str:\n",
    "    toks = tokenize_upper(cond)\n",
    "    if PUPPY_WORDS & toks: return \"puppy\"\n",
    "    if SENIOR_WORDS & toks: return \"senior\"\n",
    "    return \"adult\"\n",
    "\n",
    "def build_bert_text_column(df: pd.DataFrame) -> pd.Series:\n",
    "    parts = []\n",
    "    for col in TEXT_COLS_FOR_BERT:\n",
    "        if col in df.columns:\n",
    "            parts.append(df[col].fillna(\"\").astype(str))\n",
    "    if not parts:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index)\n",
    "    text = parts[0]\n",
    "    for extra in parts[1:]:\n",
    "        text = text + \" [SEP] \" + extra\n",
    "    return text\n",
    "\n",
    "def compute_stay_length_days(df: pd.DataFrame) -> pd.Series:\n",
    "    stay = (df[\"Outcome_Date\"] - df[\"Intake_Date\"]).dt.total_seconds() / 86400.0\n",
    "    stay = stay.round().astype(\"float\")\n",
    "    stay = stay.fillna(df.get(\"Days_in_Custody\", pd.Series([np.nan] * len(df), index=df.index)))\n",
    "    return stay\n",
    "\n",
    "def extract_intake_hour(df: pd.DataFrame) -> pd.Series:\n",
    "    dt = df[\"Intake_Date\"]\n",
    "    if not pd.api.types.is_datetime64_any_dtype(dt):\n",
    "        return pd.Series([np.nan] * len(df), index=df.index)\n",
    "    hours = dt.dt.hour.astype(\"float\")\n",
    "    mins = dt.dt.minute.astype(\"float\")\n",
    "    secs = dt.dt.second.astype(\"float\")\n",
    "    if hours.nunique(dropna=True) <= 1 and mins.nunique(dropna=True) <= 1 and secs.nunique(dropna=True) <= 1:\n",
    "        if len(hours.dropna()) > 0 and float(hours.dropna().iloc[0]) == 0.0 and float(mins.dropna().iloc[0]) == 0.0:\n",
    "            return pd.Series([np.nan] * len(df), index=df.index)\n",
    "    return hours\n",
    "\n",
    "def _hash_texts_for_cache(texts: pd.Series, model_name: str, max_length: int) -> str:\n",
    "    h = hashlib.md5()\n",
    "    h.update(model_name.encode(\"utf-8\"))\n",
    "    h.update(str(max_length).encode(\"utf-8\"))\n",
    "    n = len(texts)\n",
    "    sample = pd.concat([texts.iloc[: min(2000, n)], texts.iloc[max(0, n - 2000):]], axis=0)\n",
    "    joined = \"\\n\".join(sample.astype(str).tolist())\n",
    "    h.update(joined.encode(\"utf-8\", errors=\"ignore\"))\n",
    "    h.update(str(n).encode(\"utf-8\"))\n",
    "    return h.hexdigest()\n",
    "\n",
    "def compute_bert_embeddings(\n",
    "    texts: pd.Series,\n",
    "    device: torch.device,\n",
    "    cache_dir: str,\n",
    "    model_name: str = BERT_MODEL_NAME,\n",
    "    max_length: int = BERT_MAX_LENGTH,\n",
    "    batch_size: int = BERT_BATCH_SIZE,\n",
    "    use_amp: bool = True,\n",
    ") -> np.ndarray:\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    key = _hash_texts_for_cache(texts, model_name=model_name, max_length=max_length)\n",
    "    cache_path = os.path.join(cache_dir, f\"bert_emb_{key}.npy\")\n",
    "    if os.path.isfile(cache_path):\n",
    "        status(f\"[BERT] cache hit: {os.path.basename(cache_path)}\")\n",
    "        return np.load(cache_path).astype(np.float32)\n",
    "    status(f\"[BERT] computing embeddings on {device} ...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    all_embs = []\n",
    "    n = len(texts)\n",
    "    use_cuda_amp = (device.type == \"cuda\") and use_amp\n",
    "    t0 = time.time()\n",
    "    for start in range(0, n, batch_size):\n",
    "        batch_texts = texts.iloc[start:start + batch_size].tolist()\n",
    "        enc = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            if use_cuda_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(**enc)\n",
    "            else:\n",
    "                outputs = model(**enc)\n",
    "            cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "        all_embs.append(cls_emb.detach().cpu().numpy())\n",
    "        if (start // batch_size) % 20 == 0:\n",
    "            done = min(start + batch_size, n)\n",
    "            status(f\"[BERT] {done}/{n} done, elapsed {int(time.time() - t0)}s\")\n",
    "    embs = np.concatenate(all_embs, axis=0).astype(np.float32)\n",
    "    np.save(cache_path, embs)\n",
    "    status(f\"[BERT] saved cache: {os.path.basename(cache_path)}\")\n",
    "    return embs\n",
    "\n",
    "def build_features_and_time(df: pd.DataFrame, bert_device: torch.device, out_dir: str):\n",
    "    df = df[df[\"Animal_Type\"].astype(str).str.upper().eq(\"DOG\")].copy()\n",
    "    status(f\"[FILTER] Animal_Type==DOG rows={len(df)}\")\n",
    "    df[\"Intake_Date\"] = pd.to_datetime(df[\"Intake_Date\"], errors=\"coerce\")\n",
    "    df[\"Outcome_Date\"] = pd.to_datetime(df[\"Outcome_Date\"], errors=\"coerce\")\n",
    "    df[\"StayLength\"] = compute_stay_length_days(df)\n",
    "    df = df[df[\"StayLength\"].notna() & (df[\"StayLength\"] >= 0)].reset_index(drop=True)\n",
    "    if ADOPTION_ONLY:\n",
    "        before = len(df)\n",
    "        df = df[df[\"Outcome_Type\"].eq(\"ADOPTION\")].copy()\n",
    "        status(f\"[FILTER] ADOPTION_ONLY keep={len(df)}/{before}\")\n",
    "    status(f\"[STAY] StayLength describe:\\n{df['StayLength'].describe()}\")\n",
    "    df[\"Breed_Use\"] = df[\"Animal_Breed\"].apply(map_breed_use)\n",
    "    df[\"Breed_Size\"] = df[\"Animal_Breed\"].apply(map_breed_size)\n",
    "    df[\"Breed_Temper\"] = df[\"Animal_Breed\"].apply(map_breed_temper)\n",
    "    df[\"IsFierceBreed\"] = (df[\"Breed_Temper\"] == \"fierce\").astype(int)\n",
    "    df[\"Intake_Condition_Group\"] = df[\"Intake_Condition\"].apply(normalize_intake_condition)\n",
    "    if \"AgeGroup\" not in df.columns or df[\"AgeGroup\"].astype(str).str.strip().eq(\"\").all():\n",
    "        df[\"AgeGroup\"] = df[\"Intake_Condition\"].apply(infer_age_group_from_condition)\n",
    "    else:\n",
    "        df[\"AgeGroup\"] = df[\"AgeGroup\"].fillna(\"adult\").astype(str).str.lower()\n",
    "    df[\"AgeDays\"] = df[\"AgeGroup\"].map(AGE_DAYS_MAP).fillna(AGE_DAYS_MAP[\"adult\"])\n",
    "    df[\"AgeLog\"] = np.log1p(df[\"AgeDays\"].astype(float))\n",
    "    df[\"Intake_Month\"] = df[\"Intake_Date\"].dt.month.astype(\"Int64\").fillna(0).astype(int)\n",
    "    df[\"Intake_Year\"] = df[\"Intake_Date\"].dt.year.astype(\"Int64\").fillna(0).astype(int)\n",
    "    df[\"Intake_Weekday\"] = df[\"Intake_Date\"].dt.weekday.astype(\"Int64\").fillna(0).astype(int)\n",
    "    df[\"IsWeekend\"] = (df[\"Intake_Weekday\"] >= 5).astype(int)\n",
    "    try:\n",
    "        import holidays\n",
    "        us_holidays = holidays.US()\n",
    "        df[\"IsHoliday\"] = df[\"Intake_Date\"].dt.date.apply(lambda d: d in us_holidays if pd.notna(d) else False).astype(int)\n",
    "    except Exception:\n",
    "        df[\"IsHoliday\"] = 0\n",
    "    df[\"Intake_Hour\"] = extract_intake_hour(df)\n",
    "    def _time_of_day_bin(hour):\n",
    "        if pd.isna(hour): return \"unknown\"\n",
    "        h = int(hour)\n",
    "        if h < 6: return \"night\"\n",
    "        if h < 12: return \"morning\"\n",
    "        if h < 18: return \"afternoon\"\n",
    "        return \"evening\"\n",
    "    df[\"TimeOfDayBin\"] = df[\"Intake_Hour\"].apply(_time_of_day_bin)\n",
    "    df[\"Use__x__Health\"] = df[\"Breed_Use\"].astype(str) + \"||\" + df[\"Intake_Condition_Group\"].astype(str)\n",
    "    df[\"Size__x__Health\"] = df[\"Breed_Size\"].astype(str) + \"||\" + df[\"Intake_Condition_Group\"].astype(str)\n",
    "    df[\"Temper__x__Health\"] = df[\"Breed_Temper\"].astype(str) + \"||\" + df[\"Intake_Condition_Group\"].astype(str)\n",
    "    df[\"Weekend__x__TimeBin\"] = df[\"IsWeekend\"].astype(str) + \"||\" + df[\"TimeOfDayBin\"].astype(str)\n",
    "    df[\"Breed_Clean\"] = df[\"Animal_Breed\"].fillna(\"Unknown\").astype(str).str.upper().str.strip()\n",
    "    top_breeds = df[\"Breed_Clean\"].value_counts().index[:TOP_N_BREEDS].tolist()\n",
    "    breed_ohe_cols: List[str] = []\n",
    "    for b in top_breeds:\n",
    "        col = \"Breed_\" + re.sub(r\"[^A-Z0-9]+\", \"_\", b)[:30]\n",
    "        df[col] = (df[\"Breed_Clean\"] == b).astype(int)\n",
    "        breed_ohe_cols.append(col)\n",
    "    df[\"Breed_Other\"] = (~df[\"Breed_Clean\"].isin(top_breeds)).astype(int)\n",
    "    bert_emb_all = None\n",
    "    bert_feature_names: List[str] = []\n",
    "    if USE_BERT:\n",
    "        status(\"[BERT] building text column ...\")\n",
    "        df[\"text_for_bert\"] = build_bert_text_column(df)\n",
    "        cache_dir = os.path.join(out_dir, BERT_CACHE_DIR)\n",
    "        bert_emb_all = compute_bert_embeddings(\n",
    "            df[\"text_for_bert\"],\n",
    "            device=bert_device,\n",
    "            cache_dir=cache_dir,\n",
    "            model_name=BERT_MODEL_NAME,\n",
    "            max_length=BERT_MAX_LENGTH,\n",
    "            batch_size=BERT_BATCH_SIZE,\n",
    "            use_amp=BERT_USE_AMP,\n",
    "        )\n",
    "        bert_feature_names = [f\"BERT_{i}\" for i in range(bert_emb_all.shape[1])]\n",
    "    numeric_cols = [\n",
    "        \"Intake_Month\",\"Intake_Year\",\"Intake_Weekday\",\"IsWeekend\",\"IsHoliday\",\n",
    "        \"AgeLog\",\"IsFierceBreed\",\n",
    "    ] + breed_ohe_cols + [\"Breed_Other\"]\n",
    "    base_cat_cols = [\n",
    "        \"Breed_Use\",\"Breed_Size\",\"Breed_Temper\",\"AgeGroup\",\"Intake_Condition_Group\",\"TimeOfDayBin\",\n",
    "        \"Use__x__Health\",\"Size__x__Health\",\"Temper__x__Health\",\"Weekend__x__TimeBin\",\n",
    "    ]\n",
    "    extra_cat_cols: List[str] = []\n",
    "    if \"Intake_Type\" in df.columns:\n",
    "        extra_cat_cols.append(\"Intake_Type\")\n",
    "    cat_cols = base_cat_cols + extra_cat_cols\n",
    "    X_num_raw = df[numeric_cols].fillna(0.0).to_numpy().astype(np.float32)\n",
    "    num_mean = X_num_raw.mean(axis=0)\n",
    "    num_std = X_num_raw.std(axis=0)\n",
    "    num_std[num_std == 0] = 1.0\n",
    "    X_num = ((X_num_raw - num_mean) / num_std).astype(np.float32)\n",
    "    cat_ohe = pd.get_dummies(df[cat_cols].fillna(\"unknown\"), prefix=cat_cols, dummy_na=False)\n",
    "    X_cat = cat_ohe.to_numpy().astype(np.float32)\n",
    "    X_list = [X_num, X_cat]\n",
    "    feature_names = numeric_cols + list(cat_ohe.columns)\n",
    "    if USE_BERT and bert_emb_all is not None:\n",
    "        X_list.append(bert_emb_all)\n",
    "        feature_names += bert_feature_names\n",
    "    X_full = np.concatenate(X_list, axis=1).astype(np.float32)\n",
    "    input_dim = X_full.shape[1]\n",
    "    if input_dim > TOP_K_FS:\n",
    "        status(\"[FS] L1 Logistic feature selection (saga + n_jobs=-1) ...\")\n",
    "        lr = LogisticRegression(\n",
    "            penalty=\"l1\",\n",
    "            C=0.1,\n",
    "            solver=\"saga\",\n",
    "            max_iter=2000,\n",
    "            multi_class=\"ovr\",\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        y_fs = pd.qcut(df[\"StayLength\"], q=3, labels=False, duplicates=\"drop\").astype(int).to_numpy()\n",
    "        lr.fit(X_full, y_fs)\n",
    "        coef_abs = np.abs(lr.coef_).max(axis=0)\n",
    "        order = np.argsort(-coef_abs)\n",
    "        selected_idx = np.sort(order[:TOP_K_FS])\n",
    "        X_full = X_full[:, selected_idx]\n",
    "        feature_names = [feature_names[i] for i in selected_idx]\n",
    "        status(f\"[FS] original_dim={input_dim}, selected_dim={X_full.shape[1]}\")\n",
    "    else:\n",
    "        status(f\"[FS] no need, dim={input_dim}\")\n",
    "    return df, X_full, feature_names\n",
    "\n",
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self) -> int:\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class BinaryMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, feat_dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        hidden1 = 256\n",
    "        hidden2 = 128\n",
    "        self.feat_dropout = nn.Dropout(feat_dropout) if feat_dropout > 0 else None\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden1),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden2),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.head = nn.Linear(hidden2, 1)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.feat_dropout is not None:\n",
    "            x = self.feat_dropout(x)\n",
    "        h = self.trunk(x)\n",
    "        logit = self.head(h).squeeze(1)\n",
    "        return logit\n",
    "\n",
    "def bce_with_label_smoothing(logits: torch.Tensor, y: torch.Tensor, smoothing: float) -> torch.Tensor:\n",
    "    if smoothing and smoothing > 0:\n",
    "        y = (1.0 - smoothing) * y + 0.5 * smoothing\n",
    "    return nn.functional.binary_cross_entropy_with_logits(logits, y)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_binary(model: nn.Module, loader: DataLoader, device: torch.device) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    ys = []\n",
    "    ps = []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        ys.append(yb.numpy())\n",
    "        ps.append(prob)\n",
    "    y_true = np.concatenate(ys).astype(int)\n",
    "    p = np.concatenate(ps)\n",
    "    y_pred = (p >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, p)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "    try:\n",
    "        brier = brier_score_loss(y_true, p)\n",
    "    except ValueError:\n",
    "        brier = float(\"nan\")\n",
    "    return {\"acc\": float(acc), \"f1\": float(f1), \"auc\": float(auc), \"brier\": float(brier)}\n",
    "\n",
    "def train_one_run_binary(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    device: torch.device,\n",
    "    feat_dropout: float = 0.0,\n",
    "    use_class_weight: bool = True,\n",
    ") -> Dict[str, float]:\n",
    "    idx_all = np.arange(len(y))\n",
    "    idx_trainval, idx_test = train_test_split(idx_all, test_size=0.15, random_state=SEED, stratify=y)\n",
    "    y_trainval = y[idx_trainval]\n",
    "    idx_train, idx_val = train_test_split(\n",
    "        idx_trainval, test_size=0.15 / 0.85, random_state=SEED, stratify=y_trainval\n",
    "    )\n",
    "    X_train, y_train = X[idx_train], y[idx_train]\n",
    "    X_val, y_val = X[idx_val], y[idx_val]\n",
    "    X_test, y_test = X[idx_test], y[idx_test]\n",
    "    train_ds = BinaryDataset(X_train, y_train)\n",
    "    val_ds = BinaryDataset(X_val, y_val)\n",
    "    test_ds = BinaryDataset(X_test, y_test)\n",
    "    pin = (device.type == \"cuda\")\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=pin)\n",
    "    val_loader = DataLoader(val_ds, batch_size=2048, shuffle=False, num_workers=NUM_WORKERS, pin_memory=pin)\n",
    "    test_loader = DataLoader(test_ds, batch_size=2048, shuffle=False, num_workers=NUM_WORKERS, pin_memory=pin)\n",
    "    model = BinaryMLP(input_dim=X.shape[1], feat_dropout=feat_dropout).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    pos_weight = None\n",
    "    if use_class_weight:\n",
    "        pos = float(np.sum(y_train == 1))\n",
    "        neg = float(np.sum(y_train == 0))\n",
    "        if pos > 0:\n",
    "            pos_weight = torch.tensor([neg / max(pos, 1.0)], device=device, dtype=torch.float32)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "    best_val_auc = -1e9\n",
    "    best_state = None\n",
    "    for ep in range(EPOCHS):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "                logits = model(xb)\n",
    "                if pos_weight is None:\n",
    "                    loss = bce_with_label_smoothing(logits, yb, LABEL_SMOOTHING)\n",
    "                else:\n",
    "                    y_smooth = (1.0 - LABEL_SMOOTHING) * yb + 0.5 * LABEL_SMOOTHING\n",
    "                    loss = nn.functional.binary_cross_entropy_with_logits(logits, y_smooth, pos_weight=pos_weight)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            losses.append(float(loss.detach().cpu().item()))\n",
    "        if (ep + 1) % max(5, EPOCHS // 5) == 0:\n",
    "            val_m = eval_binary(model, val_loader, device=device)\n",
    "            status(f\"[TRAIN] ep {ep+1}/{EPOCHS} loss={np.mean(losses):.4f} | val AUC={val_m['auc']:.4f}, F1={val_m['f1']:.4f}\")\n",
    "            if val_m[\"auc\"] == val_m[\"auc\"] and val_m[\"auc\"] > best_val_auc:\n",
    "                best_val_auc = val_m[\"auc\"]\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    test_m = eval_binary(model, test_loader, device=device)\n",
    "    val_m = eval_binary(model, val_loader, device=device)\n",
    "    return {\n",
    "        \"val_acc\": val_m[\"acc\"], \"val_f1\": val_m[\"f1\"], \"val_auc\": val_m[\"auc\"], \"val_brier\": val_m[\"brier\"],\n",
    "        \"test_acc\": test_m[\"acc\"], \"test_f1\": test_m[\"f1\"], \"test_auc\": test_m[\"auc\"], \"test_brier\": test_m[\"brier\"],\n",
    "    }\n",
    "\n",
    "def run_binary_task_many_runs(\n",
    "    X: np.ndarray,\n",
    "    stay_days: np.ndarray,\n",
    "    K: int,\n",
    "    device: torch.device,\n",
    "    n_runs: int,\n",
    ") -> Dict[str, float]:\n",
    "    y = (stay_days <= K).astype(int)\n",
    "    if len(np.unique(y)) < 2:\n",
    "        raise ValueError(f\"K={K}: only one class present, cannot train.\")\n",
    "    status(f\"[TASK] K={K} days binary, positive_rate={y.mean():.4f}\")\n",
    "    metrics = []\n",
    "    for run in range(n_runs):\n",
    "        status(f\"[RUN] K={K} run {run+1}/{n_runs} start\")\n",
    "        set_seed(SEED + run)\n",
    "        m = train_one_run_binary(X, y, device=device, feat_dropout=0.0, use_class_weight=True)\n",
    "        status(f\"[RUN] K={K} run {run+1}/{n_runs} DONE | test AUC={m['test_auc']:.4f}, F1={m['test_f1']:.4f}\")\n",
    "        metrics.append(m)\n",
    "    def mean_std(key: str) -> Tuple[float, float]:\n",
    "        arr = np.array([m[key] for m in metrics], dtype=float)\n",
    "        return float(np.nanmean(arr)), float(np.nanstd(arr))\n",
    "    out = {\"K\": K}\n",
    "    for key in [\"val_acc\", \"val_f1\", \"val_auc\", \"val_brier\", \"test_acc\", \"test_f1\", \"test_auc\", \"test_brier\"]:\n",
    "        mu, sd = mean_std(key)\n",
    "        out[key + \"_mean\"] = mu\n",
    "        out[key + \"_std\"] = sd\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "    print_gpu_info()\n",
    "    df, out_dir = load_and_merge_sources(FILE_PATHS)\n",
    "    results_txt_path = os.path.join(out_dir, RESULTS_TXT_NAME)\n",
    "    with open(results_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Binary 3-task (K=7/14/30) Final Summary Only\\n\")\n",
    "    bert_device = pick_cuda_device(BERT_GPU_ID)\n",
    "    train_device = pick_cuda_device(TRAIN_GPU_ID)\n",
    "    status(f\"[DEV] bert_device={bert_device}, train_device={train_device}\")\n",
    "    df_used, X_full, feat_names = build_features_and_time(df, bert_device=bert_device, out_dir=out_dir)\n",
    "    stay_days = df_used[\"StayLength\"].to_numpy().astype(float)\n",
    "    status(f\"[DATA] rows_used={len(df_used)}, X_dim={X_full.shape[1]}\")\n",
    "    write_final_line(results_txt_path, \"\")\n",
    "    write_final_line(results_txt_path, f\"ADOPTION_ONLY={ADOPTION_ONLY}, USE_BERT={USE_BERT}, N_RUNS={N_RUNS}\")\n",
    "    write_final_line(results_txt_path, f\"rows_used={len(df_used)}, dim={X_full.shape[1]}\")\n",
    "    write_final_line(results_txt_path, \"\")\n",
    "    write_final_line(results_txt_path, \"K\\tvalAUC_mean\\tvalAUC_std\\tvalF1_mean\\tvalF1_std\\ttestAUC_mean\\ttestAUC_std\\ttestF1_mean\\ttestF1_std\\t(brier_test_mean)\")\n",
    "    all_task_summaries = []\n",
    "    t0 = time.time()\n",
    "    for K in K_LIST:\n",
    "        status(f\"[START] K={K} task begin\")\n",
    "        summ = run_binary_task_many_runs(X_full, stay_days, K=K, device=train_device, n_runs=N_RUNS)\n",
    "        all_task_summaries.append(summ)\n",
    "        write_final_line(\n",
    "            results_txt_path,\n",
    "            f\"{K}\\t\"\n",
    "            f\"{summ['val_auc_mean']:.4f}\\t{summ['val_auc_std']:.4f}\\t\"\n",
    "            f\"{summ['val_f1_mean']:.4f}\\t{summ['val_f1_std']:.4f}\\t\"\n",
    "            f\"{summ['test_auc_mean']:.4f}\\t{summ['test_auc_std']:.4f}\\t\"\n",
    "            f\"{summ['test_f1_mean']:.4f}\\t{summ['test_f1_std']:.4f}\\t\"\n",
    "            f\"{summ['test_brier_mean']:.4f}\"\n",
    "        )\n",
    "        status(f\"[END] K={K} task done\")\n",
    "    test_auc_avg = float(np.mean([s[\"test_auc_mean\"] for s in all_task_summaries]))\n",
    "    test_f1_avg = float(np.mean([s[\"test_f1_mean\"] for s in all_task_summaries]))\n",
    "    write_final_line(results_txt_path, \"\")\n",
    "    write_final_line(results_txt_path, f\"OVERALL(avg over K): testAUC_mean={test_auc_avg:.4f}, testF1_mean={test_f1_avg:.4f}\")\n",
    "    status(f\"[DONE] total elapsed {int(time.time() - t0)}s\")\n",
    "    status(f\"[DONE] final summary saved to: {results_txt_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
